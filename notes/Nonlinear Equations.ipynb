{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Systems of Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline:\n",
    "1. Review Jacobian\n",
    "2. Motivating Example\n",
    "3. Nonlinear Systems of Equation\n",
    "4. Picard's Method\n",
    "5. Newton's Method\n",
    "6. Inexact Newton Method\n",
    "7. Line Search\n",
    "8. Semi-smooth Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Jacobians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Gradients\n",
    "\n",
    "<font color=brown>**Def:**</font> Let $f:R^m \\rightarrow R$. The gradient of $f$ at $x$ in $R^m$ is a vector $g$(if it exists) that satisfy following:\n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty}\\frac{|f(x+h) - f(x) - <g, h>|}{||g||} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the gradient looks something like:\n",
    "$$\\nabla f(x)' = \\left(\n",
    "    \\frac{\\partial f}{\\partial x_{1}},\n",
    "    \\frac{\\partial f}{\\partial x_{2}},\n",
    "    \\dots,\n",
    "    \\frac{\\partial f}{\\partial x_{m}}\n",
    "  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "Find a function whose partial derivative exists but whose gradient does not\n",
    "</font>\n",
    "\n",
    "**reference:**\n",
    "https://calculus.subwiki.org/wiki/Existence_of_partial_derivatives_not_implies_differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "\n",
    "Consider for the following function:\n",
    "\n",
    "$$f(x,y) = \\begin{cases}\n",
    "  \\frac{xy}{x^2 + y^2} & \\text{if}(x,y) \\neq (0,0)\\\\    \n",
    "  0    & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "The partial derivative exist everywhere. Even in the origin. In the origin, $f_x(0,0) = 0$ and $f_y(0,0) = 0$. On the other hand $f$ is not differentiable at $(0,0)$. For example, let $v = (\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})$. Then $$\\frac{f(0 + tv) - f(0)}{t} = \\frac{1}{2t}$$\n",
    "which does not have a limit as $t \\rightarrow 0$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Jacobian\n",
    "\n",
    "<font color=brown>**Definition:**</font> Let $f:R^m \\rightarrow R^n$. The gradient of $f$ at $x \\in R^m$ is a matrix $J$(if it exists) that satisfy following:\n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty}\\frac{||f(x+h) - f(x) - Jh||_{(n)}}{||h||_{(m)}} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Question:**what do these supscripts mean\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the Jacobian looks something like,\n",
    "$$J(f;x) = \\begin{bmatrix}\n",
    "             \\frac{\\partial f_{1}}{\\partial x_{1}}   &\n",
    "\t       \\frac{\\partial f_{1}}{\\partial x_{2}} &\n",
    "               \\cdots                                &\n",
    "               \\frac{\\partial f_{1}}{\\partial x_{n}} \\\\\n",
    "\t     \\vdots                                  &\n",
    "\t       \\vdots                                &\n",
    "               \\cdots                                &\n",
    "\t       \\vdots \\\\\n",
    "             \\frac{\\partial f_{m}}{\\partial x_{1}}   &\n",
    "\t       \\frac{\\partial f_{m}}{\\partial x_{2}} &\n",
    "\t       \\cdots                                &\n",
    "\t       \\frac{\\partial f_{m}}{\\partial x_{n}}\n",
    "\t   \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "Characterize the relationship between the Jacobian and the gradient\n",
    "</font>\n",
    "\n",
    "<font color=darkgreen>**My solution:**\n",
    "$$J(f;x) = \\begin{bmatrix}\n",
    "             \\frac{\\partial f_{1}}{\\partial x_{1}}   &\n",
    "\t       \\frac{\\partial f_{1}}{\\partial x_{2}} &\n",
    "               \\cdots                                &\n",
    "               \\frac{\\partial f_{1}}{\\partial x_{n}} \\\\\n",
    "\t     \\vdots                                  &\n",
    "\t       \\vdots                                &\n",
    "               \\cdots                                &\n",
    "\t       \\vdots \\\\\n",
    "             \\frac{\\partial f_{m}}{\\partial x_{1}}   &\n",
    "\t       \\frac{\\partial f_{m}}{\\partial x_{2}} &\n",
    "\t       \\cdots                                &\n",
    "\t       \\frac{\\partial f_{m}}{\\partial x_{n}}\n",
    "\t   \\end{bmatrix} = \\begin{bmatrix} \\nabla f_1(x)' \\\\\n",
    "       \\vdots \\\\\n",
    "       \\nabla f_m(x)'\n",
    "       \\end{bmatrix}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Motivating Problem\n",
    "\n",
    "Discuss with the team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Nonlinear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1). Problem Formulation:** \n",
    "$$0 = F(x), \\,\\,\\, F:R^m \\rightarrow R^n$$\n",
    "$$x = G(x), \\,\\,\\, F:R^m \\rightarrow R^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2). Uniqueness of solution:** \n",
    "\n",
    "Linear case of Implicity Function Theorem. \n",
    "\n",
    "**reference:**   http://www.math.ucsd.edu/~jverstra/20e-lecture13.pdf\n",
    "\n",
    "Let $M \\in R^{n\\times m}$, $m > n$, $rank(M) = n$. WLOG let $M = [A\\,\\, B]$, $A \\in R^{n\\times n}$ and invertible, $B \\in R^{n \\times (m-n)}$. Then for any $x \\in R^{m-n}$ and $z = \\begin{bmatrix} -A^{-1}Bx \\\\ x \\end{bmatrix}$ satisfies $Mz = 0$\n",
    "\n",
    "IFT: Let $F: R^m \\rightarrow R^n$ continuous differentiable, $J: R^m \\rightarrow R^{n \\times m}$. Suppose $z^*$ satisfies $F(z^*) = 0$. WLOG $J(z^*) = [J_1(z^*)\\,\\,\\,J_2(z^*)]$, $J_1(z^*) \\in R^{n \\times m}$, invertible. There is a neighborhood $U \\subseteq R^{m-n}$ and continuous $g: U \\rightarrow R^n$ s.t. $x \\in U$ then $z = \\begin{bmatrix} g(x) \\\\ z\\end{bmatrix}$ satisfies $F(z) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "<br>\n",
    "1.Using IFT, when will $F(x) = 0$ have isolated solution\n",
    "<br>\n",
    "2.What happens if $F(z^*) = 0$ and $rank(J(z^*)) < min(m,n)$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "<br>\n",
    "1.When $m=n$, the neighborhood shrink to be only one point\n",
    "<br>\n",
    "2.If $rank(J(z^*)) < min(m,n)$, then $J_1(z^*)$ is not full rank and thus will be a singular matrix (Quesions....)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Picard's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Definition:**</font> When $x = G(x)$, Picard' method is $x^+ = G(x^c)$ i.e. $x_1 = G(x_0)$,$x_2 = G(x_1)$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Definition:**</font> A map $G: R^m \\rightarrow R^n$ is a contraction on a closed set $D \\subseteq R^m$ if \n",
    "1. $x\\in D$ then $G(x) \\in D$ \n",
    "2. $\\exists \\alpha \\in (0,1)$ s.t. $\\forall x, y \\in D$, $||G(x) - G(y)|| \\leq \\alpha ||x-y||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Theorem:**</font> If G is a contraction on a closed set $D \\subseteq R^m$. Then,\n",
    "1. There exists a unique $x^* \\in D$ s.t.$x^* = G(x^*)$\n",
    "2. $x_0 \\in D$ and $x_{k+1} = G(x_k)$ then, $x_k \\rightarrow x^*$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "1. Prove this result, why $D$ needs to be closed (consider limit points)\n",
    "<br>\n",
    "2. Does this result hold in arbitrary metric space? (No)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=black>**My solution:**\n",
    "\n",
    "<br>\n",
    "<font color=brown>**Theorem:**\n",
    "Let $(X, d)$ be a complete metric space, if $Y \\subseteq X$ and $Y$ is closed then $(Y, d)$ is a complete metric space. (proof skipped)\n",
    "</font> \n",
    "\n",
    "**Existence:**\n",
    "\n",
    "Define a sequence $\\{x_k\\}$ in $D$ s.t. $x_{k+1} = G(x_k)$.\n",
    "\n",
    "First let us show that $\\{x_k\\}$ is cauchy sequence.\n",
    "\n",
    "$\\forall n > m \\geq 1$ we have: \n",
    "\n",
    "$$\\begin{align}\n",
    "||x_n - x_m|| &= ||G^n(x_0) - G^m(x_0)|| \\\\\n",
    "&\\leq \\alpha||G^{n-1}(x_0) - G^{m-1}(x_0)|| \\\\\n",
    "& \\vdots \\\\\n",
    "& \\leq \\alpha^m ||G^{n-m}(x_0) - x_0|| \\\\\n",
    "& \\leq \\alpha^m (||G^{n-m}(x_0) - G^{n-m-1}(x_0)|| + ||G^{n-m-1}(x_0) - G^{n-m-2}(x_0)|| + \\cdots + ||G(x_0) - x_0||)\\\\\n",
    "& \\leq \\alpha^m(\\alpha^{n-m-1}||G(x_0) - x_0|| + \\alpha^{n-m-2}||G(x_0) - x_0||+ \\cdots + ||G(x_0) - x_0||)\\\\\n",
    "& = \\alpha^m [\\sum_{k=0}^{n-m-1}\\alpha^k]||G(x_0)-x_0|| \\\\\n",
    "& \\leq \\alpha^m [\\sum_{k=0}^{\\infty}\\alpha^k]||G(x_0)-x_0|| \\\\\n",
    "& = \\frac{\\alpha^m }{1-\\alpha}||G(x_0)-x_0|| \n",
    "\\end{align}$$\n",
    "\n",
    "Thus $\\{x_k\\}$ is Cauchy sequence. Also $(R^n, ||.||)$ is complete metric space, this implies that $(D, ||.||)$ is complete metric space from the theorem above. Thus, $\\{x_k\\}$ has a limit (definition of complete metric space.).\n",
    "Thus, $$\\exists x \\in D,\\,\\, s.t.\\,\\, \\lim_{k\\rightarrow \\infty} x_k = x$$\n",
    "\n",
    "Also $G$ is a continuous function. Thus,\n",
    "\n",
    "$$G(x) = G(\\lim_{k\\rightarrow \\infty} x_k) = \\lim_{k\\rightarrow \\infty}G(x_k) = \\lim_{k\\rightarrow \\infty} x_{k+1} = x$$\n",
    "\n",
    "**uniqueness:**\n",
    "if $\\exists x,y$ s.t. $x = G(x)$ and $y = G(y)$. Then,\n",
    "$$\\alpha ||x - y|| \\geq ||G(x) - G(y)|| = ||x - y||$$\n",
    "Contradiction, given $\\alpha < 1$\n",
    "</font>\n",
    "\n",
    "The above statement already prove the whole theorem and explain why $D$ has to be closed.\n",
    "\n",
    "This result does not hold for an arbitrary metric space. But it holds for any **complete** metric space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Example:**</font> \n",
    "$G(x) = 0.5(x + \\frac{4}{x})$, $x_0 = 10$, $x_1 = 5.2$, ..., $x_4 = 2.006$, $x_5 = 2.0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "1. Calculate $x=G(x)$ by hand, what is $G(x)$ doing\n",
    "<br>\n",
    "2. Show that G(x) is a contraction\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "<br>\n",
    "1. $x = 2$ or $x = -2$\n",
    "<br>\n",
    "2. Do we need specify $x > 0$?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Newton's Method\n",
    "\n",
    "**reference:**\n",
    "\n",
    "https://www.lakeheadu.ca/sites/default/files/uploads/77/docs/RemaniFinal.pdf\n",
    "\n",
    "https://www.math.ntnu.no/emner/TMA4123/2012v/notater/nr-systems-a4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Definition:**</font> Given $F: R^m \\rightarrow R^m$ with a continuoud Jacobian $J:R^m \\rightarrow R^{m \\times m}$. Newton's method is the following sequence:\n",
    "$$x_+ = x_c - J(x_c)^{-1}F(x_c)$$\n",
    "\n",
    "Intuition if $x^*$ s.t. $F(x^*) = 0$, $0 = F(x^*) \\simeq F(x_c) + J(x_c)(x^* - x_c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Local Convergence Theorem**\n",
    "\n",
    "Assumption: $F: R^m \\rightarrow R^m$ and a continuous Jacobian $J:R^m \\rightarrow R^{m \\times m}$. There is an $x^*$ and an $p^*$. \n",
    "\n",
    "1. $F(x^*) = 0$ and $J(x^*)$ is nonsingular.\n",
    "2. (Lipschitz Continuous) $\\exists \\gamma >0$ s.t. $\\forall x, y \\in B(x^*, p^*)$, we have $||J(x)-J(y)|| \\leq \\gamma ||x-y||$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Theorem:**</font> If the assumption holds and $||x_c - x^*|| \\leq \\min(p^*,\\frac{1}{2\\gamma||J(x^*)^{-1}||})$. Then,\n",
    "\n",
    "1. $J(x_c)$ is non-singular and $||J(x_c)^{-1}|| \\leq 2||J(x^*)^{-1}||$\n",
    "2. $e_+ = x_+ - x^*$ and $e_c = x_c - x^*$, $||e_+|| \\leq ||J(x^*)^{-1}||\\,||e_c||^2 \\gamma \\leq \\frac{||e_c||}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "1. What is the impact of the singular values of $J(x^*)$ on the \"localness\" of the result\n",
    "<br>\n",
    "2. Why is the first statement import?\n",
    "<br>\n",
    "3. Prove + interpret\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "<br>\n",
    "1. \n",
    "<br>\n",
    "2. \n",
    "<br>\n",
    "3.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Lemma:**</font> Suppose $A, B$ and $A$ is non-singular. If for some $\\epsilon \\in (0,1)$, $||AB -I|| < 1 - \\epsilon$ then B is invertible and \n",
    "\n",
    "1. $||A-B^{-1}|| \\leq (1-\\epsilon)||B^{-1}||$\n",
    "2. $||B^{-1}|| \\leq ||A||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof of theorem:**\n",
    "(1). $$\\begin{align} ||I - J(x^*)^{-1}J(x_c)|| &\\leq ||J(x^*)^{-1}(J(x^*) - J(x_c))|| \\\\\n",
    "& \\leq ||J(x^*)^{-1}||\\,||J(x^*)^{-1}(J(x^*) - J(x_c))|| \\\\\n",
    "& \\leq ||J(x^*)^{-1}||\\,\\gamma ||x^* - x_c|| \\\\\n",
    "& \\leq \\frac{||J(x^*)^{-1}||\\gamma}{2\\gamma||J(x^*)^{-1}||} \\\\\n",
    "& = \\frac{1}{2}\n",
    "\\end{align}$$\n",
    "\n",
    "By applying the lemma above, we can get:\n",
    "$$||J(x_c)^{-1}|| \\leq 2||J(x^*)^{-1}||$$\n",
    "\n",
    "(2) we have $x_+ = x_c - J(x_c)^{-1}F(x_c)$ then,\n",
    "$$\\begin{align} e_+ &= e_c - J(x_c)^{-1}F(x_c) \\\\ \n",
    "&= J(x_c)^{-1}(J(x_c)e_c - F(x_c)) \\\\\n",
    "&= J(x_c)^{-1}(J(x_c)e_c - (F(x_c) - F(x^*))) \\\\\n",
    "&= J(x_c)^{-1}(J(x_c)e_c - (\\int_{0}^{1} J(x^*+te_c)e_c dt))\n",
    "\\end{align}$$\n",
    "\n",
    "Further,\n",
    "$$\\begin{align}\n",
    "||e_+|| &= ||J(x_c)^{-1}(J(x_c)e_c - (\\int_{0}^{1} J(x^*+te_c)e_c dt))|| \\\\\n",
    "& \\leq ||J(x_c)^{-1}||\\int_{0}^{1}||J(x_c) - J(x^*+te_c)||\\,||e_c|| dt \\\\\n",
    "& \\leq ||J(x_c)^{-1}||\\,||e_c||^2 \\int_{0}^{1} \\gamma(1-t)dt \\\\\n",
    "& = \\frac{\\gamma}{2}||J(x_c)^{-1}||\\,||e_c||^2\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "What happen if I use the mean value theorem.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "<br>\n",
    "First, here is a very good summary of taylor theorem on single-variable functions, multi-variable scalar-valued functions and Multi-variable vector-valued functions:\n",
    "http://fourier.eng.hmc.edu/e176/lectures/NM/node45.html\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "We know $x_c  = x^* + e_c$, we have the following two taylor theorem equations for multi-variable scalar-valued functions (check out the notes in CS 726):\n",
    "\n",
    "$$f(x+p) = f(x) + \\int_{0}^{1}\\nabla f(x+\\gamma p)^Tp d\\gamma$$\n",
    "$$f(x+p) = f(x) + \\nabla f(x+\\gamma p)^Tp, \\,\\,\\, for\\,some\\, \\gamma \\in (0,1)$$\n",
    "\n",
    "By applying the first one we get,\n",
    "$$F(x_c) = F(x^*) + \\int_{0}^{1} J(x^*+te_c)e_c dt$$\n",
    "By applying the second one we get (this is just first one using mean value theorem),\n",
    "$$F(x_c) = F(x^*) + J(x^* + \\omega e_c)e_c,\\,\\, \\exists \\omega \\in (0,1)$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\\begin{align}\n",
    "||e_+|| &= ||J(x_c)^{-1}[J(x_c)e_c - J(x^* + \\omega e_c)e_c]|| \\\\\n",
    "& \\leq ||J(x_c)^{-1}||\\,|| J(x_c) - J(x^* + \\omega e_c) ||\\,||e_c|| \\\\\n",
    "& \\leq ||J(x_c)^{-1}||\\,||e_c||^2\\,||\\omega||\\gamma \\\\\n",
    "& \\leq ||J(x_c)^{-1}||\\,||\\omega||\\gamma\n",
    "\\end{align}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Definition: Rates of Convergence**</font>\n",
    "Let $\\{x_k\\} \\subseteq R_{>0}$ s.t. $x_k \\rightarrow 0$\n",
    "1. If $\\limsup \\frac{x_{k+1}}{x_k} = p \\in (0,1)$ $\\implies$ converges Q-linearly\n",
    "2. If $\\limsup \\frac{x_{k+1}}{x_k} = 0)$ $\\implies$ converges Q-superlinearly\n",
    "3. If $\\limsup \\frac{x_{k+1}^2}{x_k} = L > 0 $ $\\implies$ converges Q-quadratically\n",
    "4. $\\{y_k\\} \\subseteq R_{>0}$ s.t. $x_k \\leq y_k$\n",
    "then $y_k$ converges Q-linearly, Q-superlinearly, Q-quadratically $\\implies$ $x_k$ converges R-linearly, R-superlinearly, R-quadratically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Stopping Criterion:**\n",
    "\n",
    "Option 1. $||x_{k+1} - x_k||$ when is this below a threshold?\n",
    "\n",
    "Option 2. $||F(x_k)|| \\leq \\tau_a + \\tau_c ||F(x_0)||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "Using FTC, $e = x-x^*$ and Prove that if $e_0 = x_0-x^*$ and $e$ are sufficiently small\n",
    "    $$\\frac{3}{5}k(J(x^*))^{-1}\\frac{||e||}{||e_0||}\\leq \\frac{||F(x)||}{||F(x_0)||}\\leq \\frac{5}{3}k(J(x^*))\\frac{||e||}{||e_0||}$$\n",
    "    \n",
    "   <br>\n",
    "   1. How should we choos $\\tau_a$ and $\\tau_c$?\n",
    "       \n",
    "   <br>\n",
    "   2. Guiding Principles?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. In exact Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to solve:\n",
    "$$x_+ = x_c + S(= x_c - J(x_c)^{-1}F(x_c))$$\n",
    "$$J(x_c)s + F(x_c) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.$J(x_c)$ cannot be evaluated precisely:**\n",
    "$$||J_c - J(x_c)|| \\leq \\Delta_c$$\n",
    "$$J_cs + F(x_c) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.F(x_c) cannot be evaluated method:**\n",
    "$$||F_c - F(x_c)|| \\leq \\epsilon_c$$\n",
    "$$J(x_c)s + F_c = 0$$\n",
    "$$J_cs + F_c = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Might not be able to solve for s exactly:**\n",
    "$$||J_cs+F_c|| \\leq \\eta_c ||F_c||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem:** General assumption holds and $||x_c - x^*|| \\leq min(\\frac{1}{2\\gamma||J(x^*)^{-1}||}, \\rho^*)$. Then $\\exists k > 0$ s.t. $$||e_+|| \\leq k(||e_c||^2 + (\\eta_c + \\Delta_c)||e_c|| + \\epsilon_c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.Chorel Method:**\n",
    "\n",
    "Description: 1. Only compute the Jacobian every $v$ iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.Forward Difference:**\n",
    "\n",
    "Description: replace $J(x_c)$ with its numerical difference approximation\n",
    "\n",
    "Let $f$ be $R^m \\rightarrow R$ is differentiable with Lipschitz continuous gradient.($\\exists L > 0$s.t. $||\\nabla f(x) - \\nabla f(y)^T|| \\leq L||x-y||$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "Prove $|f(y+h) - f(y) - \\nabla f(y)'h| \\leq \\frac{L}{2}||h||^2$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose $h = \\epsilon e_i$, $e_i$ is standard basis vector,\n",
    "$$\\nabla f(y)^Te_i = \\frac{f(y+\\epsilon e_i)-f(y)}{\\epsilon} + \\gamma_i, \\,\\, |\\gamma| \\leq \\frac{L\\epsilon}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "1. If $f$ can be evaluated at any input with accuracy $l_fU$, $l_f > 0$. Show that the right choice of $\\epsilon \\propto \\sqrt{U}$\n",
    "        <br>\n",
    "    2. Coose a simple differentiable function and test out a bunch of $\\epsilon$ and particularly compare to $\\epsilon = \\sqrt{U}$\n",
    "        <br>\n",
    "    3. How do I do Forward Difference Approximation for $F:R^m \\rightarrow R^n$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm:** $f$:Function, $\\epsilon$:approximation size\n",
    "\n",
    "1.$g \\rightarrow$ zeros(length(x)), $I \\rightarrow$ eye(length(x))\n",
    "\n",
    "2.for $i$ = $1$:length(x)\n",
    "\n",
    "2a.$g(i) = \\frac{f(x+\\epsilon I[.,i]) - f(x)}{\\epsilon}$\n",
    "\n",
    "2b.end\n",
    "\n",
    "3.return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Jacobian Free Newton Krylov Method**\n",
    "- $J_cs + F(x) = 0$\n",
    "- iteratively\n",
    "- Forward difference for $J_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Quasi-Newton Methods** (Brayden Methods)\n",
    "- iterates: $x_-$,$x_c$,$x_+$\n",
    "- Jacobian Approximation: $J_-$,$J_c$\n",
    "\n",
    "Recall: $f: R \\rightarrow R$ the secant equation of derivative \n",
    "$$b_c = \\frac{f(x_c) - f(x_-)}{x_c - x_-} = \\frac{y}{s}$$\n",
    "\n",
    "<font color=brown>**Define:**</font>\n",
    "\n",
    "$y_c = F(x_c) - F(x_-)$, $s_c = x_c - x_-$ (recall $x_c = x_- + s_c$), $J_c = J_- + \\frac{(y_c - J_-s_c)s_c^T}{s_c^Ts_c}$. Then we will get $s$ by solving $J_cs = F(x_c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Sherman-Morrison**</font>\n",
    "\n",
    "$$(A + uv^T)^{-1} = A^{-1} - \\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "1.Verify that S-M formula is true\n",
    "        <br>\n",
    "2.Use it to explicitly express $J_c^{-1}$ in terms of $J^{-1}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "1. We only need to show $(A + uv^T)(A^{-1} - \\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}) = I$\n",
    "   $$\\begin{align}\n",
    "   (A + uv^T)(A^{-1} - \\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}) &= AA^{-1} - \\frac{AA^{-1}uv^TA^{-1}}{1+v^TA^{-1}u} + uv^TA^{-1} - \\frac{uv^TA^{-1}uv^TA^{-1}}{1+v^TA^{-1}u} \\\\\n",
    "   &= I + uv^TA^{-1} - \\frac{uv^TA^{-1}uv^TA^{-1}uv^TA^{-1}}{1+v^TA^{-1}u} \\\\\n",
    "   & = I + uv^TA^{-1} - \\frac{u(1+v^TA^{-1}u)v^TA^{-1}}{1+v^TA^{-1}u} \\\\\n",
    "   & = I\n",
    "   \\end{align}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "2. \n",
    "$$\\begin{align}\n",
    "(J_c)^{-1} &= (J_- + \\frac{(y_c - J_-s_c)s_c^T}{s_c^Ts_c})^{-1} \\\\\n",
    "&= J_-^{-1} - \\frac{J_-^{-1}(y_c - J_-s_c)\\frac{s_c^T}{s_c^Ts_c}J_-^{-1}}{1+\\frac{s_c^T}{s_c^Ts_c}J_-^{-1}(y_c - J_-s_c)} \\\\\n",
    "&= J_-^{-1} - \\frac{J_-^{-1}y_cs_c^TJ_-^{-1} - s_cs_c^TJ_-^{-1}}{s_c^TJ_-^{-1}y_c}\n",
    "\\end{align}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
