{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Systems of Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline:\n",
    "1. Review Jacobian\n",
    "2. Motivating Example\n",
    "3. Nonlinear Systems of Equation\n",
    "4. Picard's Method\n",
    "5. Newton's Method\n",
    "6. Inexact Newton Method\n",
    "7. Line Search\n",
    "8. Semi-smooth Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Jacobians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Gradients\n",
    "\n",
    "<font color=brown>**Def:**</font> Let $f:R^m \\rightarrow R$. The gradient of $f$ at $x$ in $R^m$ is a vector $g$(if it exists) that satisfy following:\n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty}\\frac{|f(x+h) - f(x) - <g, h>|}{||g||} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the gradient looks something like:\n",
    "$$\\nabla f(x)' = \\left(\n",
    "    \\frac{\\partial f}{\\partial x_{1}},\n",
    "    \\frac{\\partial f}{\\partial x_{2}},\n",
    "    \\dots,\n",
    "    \\frac{\\partial f}{\\partial x_{m}}\n",
    "  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "Find a function whose partial derivative exists but whose gradient does not\n",
    "</font>\n",
    "\n",
    "**reference:**\n",
    "https://calculus.subwiki.org/wiki/Existence_of_partial_derivatives_not_implies_differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "\n",
    "Consider for the following function:\n",
    "\n",
    "$$f(x,y) = \\begin{cases}\n",
    "  \\frac{xy}{x^2 + y^2} & \\text{if}(x,y) \\neq (0,0)\\\\    \n",
    "  0    & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "The partial derivative exist everywhere. Even in the origin. In the origin, $f_x(0,0) = 0$ and $f_y(0,0) = 0$. On the other hand $f$ is not differentiable at $(0,0)$. For example, let $v = (\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})$. Then $$\\frac{f(0 + tv) - f(0)}{t} = \\frac{1}{2t}$$\n",
    "which does not have a limit as $t \\rightarrow 0$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Jacobian\n",
    "\n",
    "<font color=brown>**Definition:**</font> Let $f:R^m \\rightarrow R^n$. The gradient of $f$ at $x \\in R^m$ is a matrix $J$(if it exists) that satisfy following:\n",
    "\n",
    "$$\\lim_{n\\rightarrow \\infty}\\frac{||f(x+h) - f(x) - Jh||_{(n)}}{||h||_{(m)}} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**Question:**what do these supscripts mean\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the Jacobian looks something like,\n",
    "$$J(f;x) = \\begin{bmatrix}\n",
    "             \\frac{\\partial f_{1}}{\\partial x_{1}}   &\n",
    "\t       \\frac{\\partial f_{1}}{\\partial x_{2}} &\n",
    "               \\cdots                                &\n",
    "               \\frac{\\partial f_{1}}{\\partial x_{n}} \\\\\n",
    "\t     \\vdots                                  &\n",
    "\t       \\vdots                                &\n",
    "               \\cdots                                &\n",
    "\t       \\vdots \\\\\n",
    "             \\frac{\\partial f_{m}}{\\partial x_{1}}   &\n",
    "\t       \\frac{\\partial f_{m}}{\\partial x_{2}} &\n",
    "\t       \\cdots                                &\n",
    "\t       \\frac{\\partial f_{m}}{\\partial x_{n}}\n",
    "\t   \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "Characterize the relationship between the Jacobian and the gradient\n",
    "</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font color=darkgreen>**My solution:**\n",
    "$$J(f;x) = \\begin{bmatrix}\n",
    "             \\frac{\\partial f_{1}}{\\partial x_{1}}   &\n",
    "\t       \\frac{\\partial f_{1}}{\\partial x_{2}} &\n",
    "               \\cdots                                &\n",
    "               \\frac{\\partial f_{1}}{\\partial x_{n}} \\\\\n",
    "\t     \\vdots                                  &\n",
    "\t       \\vdots                                &\n",
    "               \\cdots                                &\n",
    "\t       \\vdots \\\\\n",
    "             \\frac{\\partial f_{m}}{\\partial x_{1}}   &\n",
    "\t       \\frac{\\partial f_{m}}{\\partial x_{2}} &\n",
    "\t       \\cdots                                &\n",
    "\t       \\frac{\\partial f_{m}}{\\partial x_{n}}\n",
    "\t   \\end{bmatrix} = \\begin{bmatrix} \\nabla f_1(x)' \\\\\n",
    "       \\vdots \\\\\n",
    "       \\nabla f_m(x)'\n",
    "       \\end{bmatrix}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Motivating Problem\n",
    "\n",
    "Discuss with the team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Nonlinear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1). Problem Formulation:** \n",
    "$$0 = F(x), \\,\\,\\, F:R^m \\rightarrow R^n$$\n",
    "$$x = G(x), \\,\\,\\, F:R^m \\rightarrow R^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2). Uniqueness of solution:** \n",
    "\n",
    "Linear case of Implicity Function Theorem. \n",
    "\n",
    "**reference:**   http://www.math.ucsd.edu/~jverstra/20e-lecture13.pdf\n",
    "\n",
    "Let $M \\in R^{n\\times m}$, $m > n$, $rank(M) = n$. WLOG let $M = [A\\,\\, B]$, $A \\in R^{n\\times n}$ and invertible, $B \\in R^{n \\times (m-n)}$. Then for any $x \\in R^{m-n}$ and $z = \\begin{bmatrix} -A^{-1}Bx \\\\ x \\end{bmatrix}$ satisfies $Mz = 0$\n",
    "\n",
    "IFT: Let $F: R^m \\rightarrow R^n$ continuous differentiable, $J: R^m \\rightarrow R^{n \\times m}$. Suppose $z^*$ satisfies $F(z^*) = 0$. WLOG $J(z^*) = [J_1(z^*)\\,\\,\\,J_2(z^*)]$, $J_1(z^*) \\in R^{n \\times m}$, invertible. There is a neighborhood $U \\subseteq R^{m-n}$ and continuous $g: U \\rightarrow R^n$ s.t. $x \\in U$ then $z = \\begin{bmatrix} g(x) \\\\ z\\end{bmatrix}$ satisfies $F(z) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "<br>\n",
    "1.Using IFT, when will $F(x) = 0$ have isolated solution\n",
    "<br>\n",
    "2.What happens if $F(z^*) = 0$ and $rank(J(z^*)) < min(m,n)$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "<br>\n",
    "1.When $m=n$, the neighborhood shrink to be only one point\n",
    "<br>\n",
    "2.If $rank(J(z^*)) < min(m,n)$, then $J_1(z^*)$ is not full rank and thus will be a singular matrix (Quesions....)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Picard's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Definition:**</font> When $x = G(x)$, Picard' method is $x^+ = G(x^c)$ i.e. $x_1 = G(x_0)$,$x_2 = G(x_1)$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Definition:**</font> A map $G: R^m \\rightarrow R^n$ is a contraction on a closed set $D \\subseteq R^m$ if \n",
    "1. $x\\in D$ then $G(x) \\in D$ \n",
    "2. $\\exists \\alpha \\in (0,1)$ s.t. $\\forall x, y \\in D$, $||G(x) - G(y)|| \\leq \\alpha ||x-y||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Theorem:**</font> If G is a contraction on a closed set $D \\subseteq R^m$. Then,\n",
    "1. There exists a unique $x^* \\in D$ s.t.$x^* = G(x^*)$\n",
    "2. $x_0 \\in D$ and $x_{k+1} = G(x_k)$ then, $x_k \\rightarrow x^*$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "1. Prove this result, why $D$ needs to be closed (consider limit points)\n",
    "<br>\n",
    "2. Does this result hold in arbitrary metric space? (No)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=black>**My solution:**\n",
    "\n",
    "<br>\n",
    "<font color=brown>**Theorem:**\n",
    "Let $(X, d)$ be a complete metric space, if $Y \\subseteq X$ and $Y$ is closed then $(Y, d)$ is a complete metric space. (proof skipped)\n",
    "</font> \n",
    "\n",
    "**Existence:**\n",
    "\n",
    "Define a sequence $\\{x_k\\}$ in $D$ s.t. $x_{k+1} = G(x_k)$.\n",
    "\n",
    "First let us show that $\\{x_k\\}$ is cauchy sequence.\n",
    "\n",
    "$\\forall n > m \\geq 1$ we have: \n",
    "\n",
    "$$\\begin{align}\n",
    "||x_n - x_m|| &= ||G^n(x_0) - G^m(x_0)|| \\\\\n",
    "&\\leq \\alpha||G^{n-1}(x_0) - G^{m-1}(x_0)|| \\\\\n",
    "& \\vdots \\\\\n",
    "& \\leq \\alpha^m ||G^{n-m}(x_0) - x_0|| \\\\\n",
    "& \\leq \\alpha^m (||G^{n-m}(x_0) - G^{n-m-1}(x_0)|| + ||G^{n-m-1}(x_0) - G^{n-m-2}(x_0)|| + \\cdots + ||G(x_0) - x_0||)\\\\\n",
    "& \\leq \\alpha^m(\\alpha^{n-m-1}||G(x_0) - x_0|| + \\alpha^{n-m-2}||G(x_0) - x_0||+ \\cdots + ||G(x_0) - x_0||)\\\\\n",
    "& = \\alpha^m [\\sum_{k=0}^{n-m-1}\\alpha^k]||G(x_0)-x_0|| \\\\\n",
    "& \\leq \\alpha^m [\\sum_{k=0}^{\\infty}\\alpha^k]||G(x_0)-x_0|| \\\\\n",
    "& = \\frac{\\alpha^m }{1-\\alpha}||G(x_0)-x_0|| \n",
    "\\end{align}$$\n",
    "\n",
    "Thus $\\{x_k\\}$ is Cauchy sequence. Also $(R^n, ||.||)$ is complete metric space, this implies that $(D, ||.||)$ is complete metric space from the theorem above. Thus, $\\{x_k\\}$ has a limit (definition of complete metric space.).\n",
    "Thus, $$\\exists x \\in D,\\,\\, s.t.\\,\\, \\lim_{k\\rightarrow \\infty} x_k = x$$\n",
    "\n",
    "Also $G$ is a continuous function. Thus,\n",
    "\n",
    "$$G(x) = G(\\lim_{k\\rightarrow \\infty} x_k) = \\lim_{k\\rightarrow \\infty}G(x_k) = \\lim_{k\\rightarrow \\infty} x_{k+1} = x$$\n",
    "\n",
    "**uniqueness:**\n",
    "if $\\exists x,y$ s.t. $x = G(x)$ and $y = G(y)$. Then,\n",
    "$$\\alpha ||x - y|| \\geq ||G(x) - G(y)|| = ||x - y||$$\n",
    "Contradiction, given $\\alpha < 1$\n",
    "</font>\n",
    "\n",
    "The above statement already prove the whole theorem and explain why $D$ has to be closed.\n",
    "\n",
    "This result does not hold for an arbitrary metric space. But it holds for any **complete** metric space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Example:**</font> \n",
    "$G(x) = 0.5(x + \\frac{4}{x})$, $x_0 = 10$, $x_1 = 5.2$, ..., $x_4 = 2.006$, $x_5 = 2.0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "1. Calculate $x=G(x)$ by hand, what is $G(x)$ doing\n",
    "<br>\n",
    "2. Show that G(x) is a contraction\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "<br>\n",
    "1. $x = 2$ or $x = -2$\n",
    "<br>\n",
    "2. Do we need specify $x > 0$?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Newton's Method\n",
    "\n",
    "**reference:**\n",
    "\n",
    "https://www.lakeheadu.ca/sites/default/files/uploads/77/docs/RemaniFinal.pdf\n",
    "\n",
    "https://www.math.ntnu.no/emner/TMA4123/2012v/notater/nr-systems-a4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Definition:**</font> Given $F: R^m \\rightarrow R^m$ with a continuoud Jacobian $J:R^m \\rightarrow R^{m \\times m}$. Newton's method is the following sequence:\n",
    "$$x_+ = x_c - J(x_c)^{-1}F(x_c)$$\n",
    "\n",
    "Intuition if $x^*$ s.t. $F(x^*) = 0$, $0 = F(x^*) \\simeq F(x_c) + J(x_c)(x^* - x_c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Local Convergence Theorem**\n",
    "\n",
    "Assumption: $F: R^m \\rightarrow R^m$ and a continuous Jacobian $J:R^m \\rightarrow R^{m \\times m}$. There is an $x^*$ and an $p^*$. \n",
    "\n",
    "1. $F(x^*) = 0$ and $J(x^*)$ is nonsingular.\n",
    "2. (Lipschitz Continuous) $\\exists \\gamma >0$ s.t. $\\forall x, y \\in B(x^*, p^*)$, we have $||J(x)-J(y)|| \\leq \\gamma ||x-y||$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Theorem:**</font> If the assumption holds and $||x_c - x^*|| \\leq \\min(p^*,\\frac{1}{2\\gamma||J(x^*)^{-1}||})$. Then,\n",
    "\n",
    "1. $J(x_c)$ is non-singular and $||J(x_c)^{-1}|| \\leq 2||J(x^*)^{-1}||$\n",
    "2. $e_+ = x_+ - x^*$ and $e_c = x_c - x^*$, $||e_+|| \\leq ||J(x^*)^{-1}||\\,||e_c||^2 \\gamma \\leq \\frac{||e_c||}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "1. What is the impact of the singular values of $J(x^*)$ on the \"localness\" of the result\n",
    "<br>\n",
    "2. Why is the first statement important?\n",
    "<br>\n",
    "3. Prove + interpret\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "<br>\n",
    "1. It impact the difficulty of invert J(x^*)\n",
    "<br>\n",
    "2. This guarantee that one ove the smallest sigular value does not explode.\n",
    "    We control the smalles sigular value in a good way.\n",
    "<br>\n",
    "3. see below\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Lemma:**</font> Suppose $A, B$ and $A$ is non-singular. If for some $\\epsilon \\in (0,1)$, $||AB -I|| < 1 - \\epsilon$ then B is invertible and \n",
    "\n",
    "1. $||A-B^{-1}|| \\leq (1-\\epsilon)||B^{-1}||$\n",
    "2. $||B^{-1}|| \\leq ||A||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof of theorem:**\n",
    "(1). $$\\begin{align} ||I - J(x^*)^{-1}J(x_c)|| &\\leq ||J(x^*)^{-1}(J(x^*) - J(x_c))|| \\\\\n",
    "& \\leq ||J(x^*)^{-1}||\\,||J(x^*)^{-1}(J(x^*) - J(x_c))|| \\\\\n",
    "& \\leq ||J(x^*)^{-1}||\\,\\gamma ||x^* - x_c|| \\\\\n",
    "& \\leq \\frac{||J(x^*)^{-1}||\\gamma}{2\\gamma||J(x^*)^{-1}||} \\\\\n",
    "& = \\frac{1}{2}\n",
    "\\end{align}$$\n",
    "\n",
    "By applying the lemma above, we can get:\n",
    "$$||J(x_c)^{-1}|| \\leq 2||J(x^*)^{-1}||$$\n",
    "\n",
    "(2) we have $x_+ = x_c - J(x_c)^{-1}F(x_c)$ then,\n",
    "$$\\begin{align} e_+ &= e_c - J(x_c)^{-1}F(x_c) \\\\ \n",
    "&= J(x_c)^{-1}(J(x_c)e_c - F(x_c)) \\\\\n",
    "&= J(x_c)^{-1}(J(x_c)e_c - (F(x_c) - F(x^*))) \\\\\n",
    "&= J(x_c)^{-1}(J(x_c)e_c - (\\int_{0}^{1} J(x^*+te_c)e_c dt))\n",
    "\\end{align}$$\n",
    "\n",
    "Further,\n",
    "$$\\begin{align}\n",
    "||e_+|| &= ||J(x_c)^{-1}(J(x_c)e_c - (\\int_{0}^{1} J(x^*+te_c)e_c dt))|| \\\\\n",
    "& \\leq ||J(x_c)^{-1}||\\int_{0}^{1}||J(x_c) - J(x^*+te_c)||\\,||e_c|| dt \\\\\n",
    "& \\leq ||J(x_c)^{-1}||\\,||e_c||^2 \\int_{0}^{1} \\gamma(1-t)dt \\\\\n",
    "& = \\frac{\\gamma}{2}||J(x_c)^{-1}||\\,||e_c||^2\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "What happen if I use the mean value theorem.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**My solution:**\n",
    "    \n",
    "    **the following solution is wrong. We dont have mean value theorem for multi-variable vector-valued functions.**\n",
    "    **i.e. the following equality is wrong:**\n",
    "    **$$F(x_c) = F(x^*) + J(x^* + \\omega e_c)e_c,\\,\\, \\exists \\omega \\in (0,1)$$**\n",
    "    **the following equality is correct:**\n",
    "    **$$F(x_c) = F(x^*) + \\int_{0}^{1} J(x^*+te_c)e_c dt$$**\n",
    " </font>   \n",
    "    <font color=darkgreen>\n",
    "<br>\n",
    "First, here is a very good summary of taylor theorem on single-variable functions, multi-variable scalar-valued functions and Multi-variable vector-valued functions:\n",
    "http://fourier.eng.hmc.edu/e176/lectures/NM/node45.html\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "We know $x_c  = x^* + e_c$, we have the following two taylor theorem equations for multi-variable scalar-valued functions (check out the notes in CS 726):\n",
    "\n",
    "$$f(x+p) = f(x) + \\int_{0}^{1}\\nabla f(x+\\gamma p)^Tp d\\gamma$$\n",
    "$$f(x+p) = f(x) + \\nabla f(x+\\gamma p)^Tp, \\,\\,\\, for\\,some\\, \\gamma \\in (0,1)$$\n",
    "\n",
    "By applying the first one we get,\n",
    "$$F(x_c) = F(x^*) + \\int_{0}^{1} J(x^*+te_c)e_c dt$$\n",
    "By applying the second one we get (this is just first one using mean value theorem),\n",
    "$$F(x_c) = F(x^*) + J(x^* + \\omega e_c)e_c,\\,\\, \\exists \\omega \\in (0,1)$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\\begin{align}\n",
    "||e_+|| &= ||J(x_c)^{-1}[J(x_c)e_c - J(x^* + \\omega e_c)e_c]|| \\\\\n",
    "& \\leq ||J(x_c)^{-1}||\\,|| J(x_c) - J(x^* + \\omega e_c) ||\\,||e_c|| \\\\\n",
    "& \\leq ||J(x_c)^{-1}||\\,||e_c||^2\\,||\\omega||\\gamma \\\\\n",
    "& \\leq ||J(x_c)^{-1}||\\,||\\omega||\\gamma\n",
    "\\end{align}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Definition: Rates of Convergence**</font>\n",
    "Let $\\{x_k\\} \\subseteq R_{>0}$ s.t. $x_k \\rightarrow 0$\n",
    "1. If $\\limsup \\frac{x_{k+1}}{x_k} = p \\in (0,1)$ $\\implies$ converges Q-linearly\n",
    "2. If $\\limsup \\frac{x_{k+1}}{x_k} = 0)$ $\\implies$ converges Q-superlinearly\n",
    "3. If $\\limsup \\frac{x_{k+1}}{x_k^2} = L > 0 $ $\\implies$ converges Q-quadratically\n",
    "4. $\\{y_k\\} \\subseteq R_{>0}$ s.t. $x_k \\leq y_k$\n",
    "then $y_k$ converges Q-linearly, Q-superlinearly, Q-quadratically $\\implies$ $x_k$ converges R-linearly, R-superlinearly, R-quadratically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "1. Prove that newton method is Q-linear\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Stopping Criterion:**\n",
    "\n",
    "Option 1. $||x_{k+1} - x_k||$ when is this below a threshold?\n",
    "\n",
    "Option 2. $||F(x_k)|| \\leq \\tau_a + \\tau_c ||F(x_0)||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "Using FTC, $e = x-x^*$ and Prove that if $e_0 = x_0-x^*$ and $e$ are sufficiently small\n",
    "    $$\\frac{3}{5}\\kappa(J(x^*))^{-1}\\frac{||e||}{||e_0||}\\leq \\frac{||F(x)||}{||F(x_0)||}\\leq \\frac{5}{3}\\kappa(J(x^*))\\frac{||e||}{||e_0||}$$\n",
    "    \n",
    "   <br>\n",
    "   1. How should we choos $\\tau_a$ and $\\tau_c$?\n",
    "       \n",
    "   <br>\n",
    "   2. Guiding Principles?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "proof in the paper\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. In exact Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to solve:\n",
    "$$x_+ = x_c + S(= x_c - J(x_c)^{-1}F(x_c))$$\n",
    "$$J(x_c)s + F(x_c) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.$J(x_c)$ cannot be evaluated precisely:**\n",
    "$$||J_c - J(x_c)|| \\leq \\Delta_c$$\n",
    "$$J_cs + F(x_c) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.F(x_c) cannot be evaluated method:**\n",
    "$$||F_c - F(x_c)|| \\leq \\epsilon_c$$\n",
    "$$J(x_c)s + F_c = 0$$\n",
    "$$J_cs + F_c = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Might not be able to solve for s exactly:**\n",
    "$$||J_cs+F_c|| \\leq \\eta_c ||F_c||$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem:** General assumption holds and $||x_c - x^*|| \\leq min(\\frac{1}{2\\gamma||J(x^*)^{-1}||}, \\rho^*)$. Then $\\exists k > 0$ s.t. $$||e_+|| \\leq k(||e_c||^2 + (\\eta_c + \\Delta_c)||e_c|| + \\epsilon_c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.Chorel Method:**\n",
    "\n",
    "Description: 1. Only compute the Jacobian every $v$ iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.Forward Difference:**\n",
    "\n",
    "Description: replace $J(x_c)$ with its numerical difference approximation\n",
    "\n",
    "Let $f$ be $R^m \\rightarrow R$ is differentiable with Lipschitz continuous gradient.($\\exists L > 0$s.t. $||\\nabla f(x) - \\nabla f(y)^T|| \\leq L||x-y||$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "Prove $|f(y+h) - f(y) - \\nabla f(y)'h| \\leq \\frac{L}{2}||h||^2$\n",
    "</font>\n",
    "\n",
    "**Might be a good proof for the mean value theorem on Hessian Matrix:**\n",
    "\n",
    "https://math.stackexchange.com/questions/2023654/use-mean-value-theorem-to-show-fy-fx-nabla-fxty-x-int-limits-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "Just use the second order Taylor theorem:\n",
    "    $$f(x+p) = f(x) + \\nabla f(x)^Tp + \\frac{1}{2}p^T \\nabla^2f(x+\\gamma p)p, \\,\\,\\gamma \\in (0,1)$$\n",
    "Thus,\n",
    "    $$f(y+h) - f(y) - \\nabla f(y)^Th = \\frac{1}{2}h^T \\nabla^2f(y+\\gamma h)h = \n",
    "    \\int_{0}^{1}[\\nabla f(x+t\\gamma h) - \\nabla f(x)]^T \\gamma hdt$$\n",
    "Then take norm on both side and apply lipschitz continous will get the desired result.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "To see why the second equation is true, set $g(t) = \\nabla f(x+th)^Th$, then\n",
    "\n",
    "$\\exists t^* \\in (0,t) $ s.t. \n",
    "\n",
    "$$t g^\\prime(t^*) = g(t) -g(0) = \\nabla f(x+th)^Th - \\nabla f(x)^Th = [\\nabla f(x+th) - \\nabla f(x)]^Th$$\n",
    "Next Let us compute $g^\\prime(t)$ :\n",
    "$$\\begin{align}\n",
    "g^\\prime(t) &= \\frac{d}{dt}[\\nabla f(x+th)^Th] = \\frac{d}{dt}[\\sum_{i=1}^{n}\\frac{\\partial f(x + th)}{\\partial x_i}h_i] = \\sum_{i=1}^{n} [\\frac{d}{dt}\\frac{\\partial f(x + th)}{\\partial x_i}h_i] \\\\\n",
    "&= \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\frac{\\partial^2f(x + th)}{\\partial x_i \\partial x_j}h_jh_i\n",
    "\\end{align}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Important things about gradient and hessian**</font>\n",
    "\n",
    "**Important reference:**\n",
    "\n",
    "**https://math.stackexchange.com/questions/2023654/use-mean-value-theorem-to-show-fy-fx-nabla-fxty-x-int-limits-0**\n",
    "\n",
    "**Jorge Nocedal, Stephen Wright - Numerical Optimization.(Appendx and Exerecise 2.10)**\n",
    "\n",
    "When the vector $x$ in turn depends on another vector $t$ (that is, $x = x(t)$), we can extend the normal chain rule for the univariate function. Defining:\n",
    "\n",
    "$$h(t) = f(x(t))$$\n",
    "we have\n",
    "$$\\nabla h(t) = \\sum_{i=1}^{n}\\frac{\\partial f}{\\partial x_i}\\nabla x_i(t) = \\nabla x(t) \\nabla f(x(t))$$\n",
    "\n",
    "Special cases of the chain rule can be derived when $x(t)$ is a linear function of $t$, say $x(t) = Ct$. We then have $\\nabla x(t) = C^T$, so that \n",
    "\n",
    "$$\\nabla h(t) = C^T \\nabla f(Ct)$$\n",
    "In the case in which f is a scalar function, we can differentiate twice using the chain rule to obtain\n",
    "$$\\nabla^2h(t) = C^T\\nabla^2f(Ct)C$$\n",
    "\n",
    "Look at exercise 2.10 in (NW06) Numerical Optimization:\n",
    "\n",
    "Suppose that $\\tilde{f}(z) = f(x)$, where $x = Sz + s$ for some $S \\in \\mathbb{R}^{n \\times n}$ and $s \\in \\mathbb{R}^n$. show that $\\nabla \\tilde{f}(z) = S^T \\nabla f(x)$, $\\nabla^2\\tilde{f}(z) = S^T \\nabla^2f(x) S$.\n",
    "\n",
    "Solution:\n",
    "\n",
    "Note first that \n",
    "$$x_j = \\sum_{i=1}^{n}S_{ji}z_i + s_j$$\n",
    "By the chain rule we have \n",
    "$$  \\frac{\\partial}{\\partial z_i} \\tilde{f}(z) =  \\sum_{j=1}^{n} \\frac{\\partial f}{\\partial x_j} \\frac{\\partial x_j}{\\partial z_i} = \\sum_{j=1}^{n} S_{ji} \\frac{\\partial f}{\\partial x_j} = [S^T \\nabla f(x)]_i$$\n",
    "For the second derivatives, we apply the chain rule again\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial^2}{\\partial z_i \\partial z_k} \\tilde{f}(z) = \\frac{\\partial}{\\partial z_k}\\sum_{j=1}^{n} S_{ji} \\frac{\\partial f}{\\partial x_j} = \\sum_{j=1}^{n} \\sum_{l=1}^{n} S_{ji} \\frac{\\partial^2f(x)}{\\partial x_j \\partial x_l} \\frac{x_l}{\\partial z_k} S_{lk} = [S^T \\nabla^2f(x) S]_{ki}\n",
    "\\end{align}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Choose $h = \\epsilon e_i$, $e_i$ is standard basis vector,\n",
    "$$\\nabla f(y)^Te_i = \\frac{f(y+\\epsilon e_i)-f(y)}{\\epsilon} + \\gamma_i, \\,\\, |\\gamma| \\leq \\frac{L\\epsilon}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "1. If $f$ can be evaluated at any input with accuracy $l_fU$, $l_f > 0$. Show that the right choice of $\\epsilon \\propto \\sqrt{U}$\n",
    "        <br>\n",
    "    2. Coose a simple differentiable function and test out a bunch of $\\epsilon$ and particularly compare to $\\epsilon = \\sqrt{U}$\n",
    "        <br>\n",
    "    3. How do I do Forward Difference Approximation for $F:R^m \\rightarrow R^n$\n",
    "    (**reference: Numerical Optimization Page 198**)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "    <br>\n",
    "    **reference: Numerical Optimization P 196**\n",
    "</font>\n",
    "\n",
    "$$|\\text{comp}(f(x)) - f(x)| \\leq {u}L_f$$\n",
    "$$|\\text{comp}(f(x + \\epsilon e_i)) - f(x + \\epsilon e_i)| \\leq {u}L_f$$\n",
    "If we use these computed values of $f$ in place of the exact values in \n",
    "$$\\nabla f(y)^Te_i = \\frac{f(y+\\epsilon e_i)-f(y)}{\\epsilon} + \\gamma_i, \\,\\, |\\gamma| \\leq \\frac{L\\epsilon}{2}$$\n",
    "\n",
    "we obtain an error that is bounded by \n",
    "\n",
    "$$ (L/2)\\epsilon + 2uL_f/\\epsilon$$\n",
    "\n",
    "Naturally, we would like to choose $\\epsilon$ to make this error as small as possible; it is easy to see that the minimizing value is $$\\epsilon^2 = \\frac{4L_fu}{L}$$\n",
    "\n",
    "further, we can conclude that the following choice of $\\epsilon$ is fairly close to optimal if the problem is well scaled. $$\\epsilon = \\sqrt{u}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm:** $f$:Function, $\\epsilon$:approximation size\n",
    "\n",
    "1.$g \\rightarrow$ zeros(length(x)), $I \\rightarrow$ eye(length(x))\n",
    "\n",
    "2.for $i$ = $1$:length(x)\n",
    "\n",
    "2a.$g(i) = \\frac{f(x+\\epsilon I[.,i]) - f(x)}{\\epsilon}$\n",
    "\n",
    "2b.end\n",
    "\n",
    "3.return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Jacobian Free Newton Krylov Method**\n",
    "- $J_cs + F(x) = 0$\n",
    "- iteratively\n",
    "- Forward difference for $J_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D. Quasi-Newton Methods** (Brayden Methods)\n",
    "- iterates: $x_-$,$x_c$,$x_+$\n",
    "- Jacobian Approximation: $J_-$,$J_c$\n",
    "\n",
    "Recall: $f: R \\rightarrow R$ the secant equation of derivative \n",
    "$$b_c = \\frac{f(x_c) - f(x_-)}{x_c - x_-} = \\frac{y}{s}$$\n",
    "\n",
    "<font color=brown>**Define:**</font>\n",
    "\n",
    "$y_c = F(x_c) - F(x_-)$, $s_c = x_c - x_-$ (recall $x_c = x_- + s_c$), $J_c = J_- + \\frac{(y_c - J_-s_c)s_c^T}{s_c^Ts_c}$. Then we will get $s$ by solving $J_cs = F(x_c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Sherman-Morrison**</font>\n",
    "\n",
    "$$(A + uv^T)^{-1} = A^{-1} - \\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "    <br>\n",
    "1.Verify that S-M formula is true\n",
    "        <br>\n",
    "2.Use it to explicitly express $J_c^{-1}$ in terms of $J^{-1}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "1. We only need to show $(A + uv^T)(A^{-1} - \\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}) = I$\n",
    "   $$\\begin{align}\n",
    "   (A + uv^T)(A^{-1} - \\frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}) &= AA^{-1} - \\frac{AA^{-1}uv^TA^{-1}}{1+v^TA^{-1}u} + uv^TA^{-1} - \\frac{uv^TA^{-1}uv^TA^{-1}}{1+v^TA^{-1}u} \\\\\n",
    "   &= I + uv^TA^{-1} - \\frac{uv^TA^{-1}uv^TA^{-1}uv^TA^{-1}}{1+v^TA^{-1}u} \\\\\n",
    "   & = I + uv^TA^{-1} - \\frac{u(1+v^TA^{-1}u)v^TA^{-1}}{1+v^TA^{-1}u} \\\\\n",
    "   & = I\n",
    "   \\end{align}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "2. \n",
    "$$\\begin{align}\n",
    "(J_c)^{-1} &= (J_- + \\frac{(y_c - J_-s_c)s_c^T}{s_c^Ts_c})^{-1} \\\\\n",
    "&= J_-^{-1} - \\frac{J_-^{-1}(y_c - J_-s_c)\\frac{s_c^T}{s_c^Ts_c}J_-^{-1}}{1+\\frac{s_c^T}{s_c^Ts_c}J_-^{-1}(y_c - J_-s_c)} \\\\\n",
    "&= J_-^{-1} - \\frac{J_-^{-1}y_cs_c^TJ_-^{-1} - s_cs_c^TJ_-^{-1}}{s_c^TJ_-^{-1}y_c}\n",
    "\\end{align}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Semi Smooth Newton Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Lipschitz Function**\n",
    "\n",
    "<font color=blue>**HW Problem:**\n",
    "Let $F(x)$ be continuous everywhere differentiable s.t. in any ball(closed), the derivative is bounded. Show that in any closed ball $F$ is Lipschitz Continuous, i.e. \n",
    "    $$\\forall B(closed),\\,\\,\\exists L>0, \\forall x,y \\in B \\,\\, ||F(x)-F(y)|| \\leq L||x-y||$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "Just use the Taylor theorem and Let $L = \\sup_{\\omega \\in B}||\\nabla F(\\omega)||$:\n",
    "    \n",
    "    $$F(x) = F(y) + \\nabla F(y + \\gamma(y-x))^T(y-x),\\,\\, for\\,\\, some\\,\\, \\gamma \\in (0,1)$$\n",
    "    $$\\begin{align} ||F(x)-F(y)|| &= ||\\nabla F(y + \\gamma(y-x))^T(y-x)|| \\\\\n",
    "    & \\leq ||\\nabla F(y + \\gamma(y-x))||\\,||(y-x)|| \\\\\n",
    "    & \\leq \\sup_{\\omega \\in B}||\\nabla F(\\omega)||\\,||(y-x)|| \\\\\n",
    "    & \\leq L||(y-x)||\n",
    "    \\end{align}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Theorem(Rademacher):**</font>\n",
    "Suppose $F:\\mathbb{R}^m \\rightarrow \\mathbb{R}^m$ is Lipschitz continuous in a ball. Then $F$ is almost everywhere(a.e.)(Lebesgue) differentiable in the ball."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Generalized Derivatives**\n",
    "\n",
    "<font color=brown>**Definition:**</font> $F:\\mathbb{R}^m \\rightarrow \\mathbb{R}^m$. Let $D_F$ denote the points at which $F$ is differentiable. The generalized derivative at $U \\in \\mathbb{R}$, $\\partial F(u)$ is a set: \n",
    "1. Let $S_u = \\{G: {u^t}\\subseteq D_F, \\lim_{t \\to \\infty}u^t = u, \\lim_{t \\to \\infty} J(u^t) = G \\}$\n",
    "2. $\\partial F(u) = cl(conv(S_u))$, closure of the convex hull of $S_u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** <font color=red>**Question:**May need to talk about it with the team\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Lemma:**</font> If a function is locally Lipschitz in a neighborhood of $u$ then $\\partial F(u) \\neq \\emptyset$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Semismooth Functions**\n",
    "\n",
    "<font color=brown>**Definition:**</font> $F$ is semismooth at $x \\in \\mathbb{R}^m$ if \n",
    "1. $F$ is locally lipschitz continuous\n",
    "2. For all $\\epsilon >0$ there is a ball around $x$, $B$ s.t. $\\forall u \\in B$ and $\\forall V \\in \\partial F(u)$ $||F(u) -F(x) - V(u-x)|| < \\epsilon ||u-x||$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**HW Problem:**\n",
    "<br>\n",
    "1. $f(x) = |x|$ is semismooth?\n",
    "<br>\n",
    "2. $f(x) = log(1 + |x|) is semismooth?$\n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>**My solution:**\n",
    "1. maybe yes?\n",
    "2. log(1 + |x|) is not locally Lipschitz continuous\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**assumption:** $F:\\mathbb{R}^m \\rightarrow \\mathbb{R}^m$ $\\exists x^*$ and $\\exists \\rho^* >0$ s.t.\n",
    "1. $F(x^*) = 0$\n",
    "2. $F$ is semismooth in $B(x^*, \\rho^*)$\n",
    "3. $\\forall V \\in \\partial F(x^*)$, $V$ is invertible\n",
    "\n",
    "**comparison:** $F:\\mathbb{R}^m \\rightarrow \\mathbb{R}^m$, continuous Jacobian, $\\exists x^*$ and $\\exists \\rho^* >0$ s.t.\n",
    "1. $F(x^*) = 0$\n",
    "2. $\\exists \\gamma >0$, $\\forall x,y \\in B(x^*, \\rho^*)$, $||J(x)-J(y)|| \\leq \\gamma ||x-y||$ \n",
    "3. $J(x^*)$ is invertible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=brown>**Theorem:**</font> Suppose the above assumption holds\n",
    "1. $\\exists k >0$ and $\\exists N$ of $x^*$ s.t. for any $x\\in N$, $V \\in \\partial F(x)$ is invertible and $||V^{-1}||\\leq k$\n",
    "2. If $x_c$ is sufficiently close to $x^*$, then $\\exists \\alpha \\in (0,1)$ for $x_+ = x_c - V_c^{-1}F(x_c)$, $||e_+|| \\leq \\alpha ||e_c||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Amijo Backtracking\n",
    "\n",
    "Previous:\n",
    "\n",
    "$x_+ = x_c + s$, $s = -J(x_c)^{-1}F(x_c)$\n",
    "\n",
    "Further:\n",
    "\n",
    "$x_+ = x_c + \\alpha d$, $d = -J(x_c)^{-1}F(x_c)$\n",
    "\n",
    "Find a $\\alpha$ s.t. $||F(x_c + \\alpha d)|| < (1-z\\alpha)||F(x)||$, usually $z = 10^{-4}$\n",
    "\n",
    "Start by $\\alpha = 1$ we test if it works, done!\n",
    "\n",
    "else set $\\alpha = \\rho \\alpha$, where $\\rho \\in [\\frac{1}{10}, \\frac{1}{2}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
